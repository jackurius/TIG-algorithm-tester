{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "379663cb-08b5-4157-8929-1790a7c0a860",
   "metadata": {},
   "source": [
    "# Local TIG Algorithm Tester"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab5e85e-efe0-4d68-b001-b44e7a84f23b",
   "metadata": {},
   "source": [
    "Tests algorithms available for benchmarking in the current round on local compute, to determine the most optimal algorithm for benchmarking\n",
    "Settings:\n",
    "- Length of time testing each algorithm (Default 30s)\n",
    "- Difficulty scaling (Default minimum scaled frontier)\n",
    "- Number of cores used to test (Default 1 core)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da1b0cda-7c2c-4cbe-9194-9f406ba50bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing modules\n",
    "\n",
    "import ipywidgets as widgets\n",
    "import requests\n",
    "import urllib\n",
    "import re\n",
    "import multiprocessing\n",
    "from tools import run_test, run_batch, build_worker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6247367f-5a37-4628-a213-50bb2c11b7df",
   "metadata": {},
   "source": [
    "## Pulling Round Data\n",
    "If an error shows, run again as sometimes the server is not as cooperative until \"Complete\" is printed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8163550-0484-4909-b1db-0701f8c627ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pulls data from the current block and round\n",
    "url = \"https://mainnet-api.tig.foundation/\"\n",
    "block_data = requests.get(f\"{url}get-block\").json()\n",
    "latest_block = block_data[\"block\"][\"id\"]\n",
    "\n",
    "challenge_data = requests.get(f\"{url}get-challenges?block_id={latest_block}\").json()\n",
    "algorithm_data = requests.get(f\"{url}get-algorithms?block_id={latest_block}\").json()\n",
    "\n",
    "challenge_id_dict = {challenge[\"id\"]:[challenge[\"details\"][\"name\"]] for challenge in challenge_data[\"challenges\"]}\n",
    "order_dict = {challenge[\"id\"]:i for (i, challenge) in enumerate(challenge_data[\"challenges\"])}\n",
    "algorithm_id_dict = {algorithm[\"id\"]:algorithm[\"details\"][\"name\"] for algorithm in algorithm_data[\"algorithms\"] if algorithm[\"state\"][\"round_pushed\"]}\n",
    "[challenge_id_dict[algorithm[\"id\"][:4]].append(algorithm[\"id\"]) for algorithm in algorithm_data[\"algorithms\"] if algorithm[\"state\"][\"round_pushed\"]]\n",
    "\n",
    "print(\"Complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c57f04-2199-4f17-801b-a6a262d2efcd",
   "metadata": {},
   "source": [
    "## Testing Algorithms\n",
    "\n",
    "Tests every algorithm for each challenge, and returns the algorithm with the lowest time per solution for each challenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7754bfc3-6ebc-4803-9108-a3d6ce5812fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select time in seconds to test each algorithm within range (10, 900)\n",
    "\n",
    "testing_time = widgets.interactive(lambda x=30:x, x=(10,900,10))\n",
    "display(testing_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9609c747-d482-4df4-b6bc-d88d26dbf53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select difficulty to be minimum in current scaled frontier or suggested algorithm testing difficulties (much lower difficulty, expecting more results)\n",
    "#WARNING lower difficulty may benefit \"greedy\" algorithms that may not perform as well at higher difficulties\n",
    "unscaled_difficulties = {\"satisfiability\":[50, 300], \"vehicle_routing\":[40, 250], \"knapsack\":[50, 10]}\n",
    "\n",
    "difficulty_selector = widgets.interactive(lambda x:x, x=[(\"Scaled difficulty\", \"scaled\"), (\"Lower difficulty\", \"unscaled\")])\n",
    "display(difficulty_selector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5de516-b4e3-473e-9ccd-f07d7e20f515",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select the number of cores you want to use during the tests (out of your maximum cores)\n",
    "#Cores are used to batch test algorithms, reducing the overall time and allowing longer timers son each algorithm test in the same overall time\n",
    "num_cores = widgets.interactive(lambda x=1:x, x=(1, multiprocessing.cpu_count(),1))\n",
    "display(num_cores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62da7d81-686c-4e90-8cf4-d52e9574d667",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run this cell until \"Complete\" is printed out\n",
    "%env RUSTFLAGS=-Awarnings\n",
    "%env TIMER={float(testing_time.result*1000)}\n",
    "\n",
    "with open(\"test_results.txt\", \"w\") as file:\n",
    "    file.write(\"New test\\n\")\n",
    "\n",
    "build_worker()\n",
    "\n",
    "for challenge in challenge_id_dict.keys():\n",
    "    \n",
    "    #Set challenge environment\n",
    "    %env CHALLENGE={challenge_id_dict[challenge][0]}\n",
    "\n",
    "    #Set difficulty to minimum in scaled frontiers and dumping into json file\n",
    "    if difficulty_selector.result == \"scaled\":\n",
    "        difficulty = sorted(challenge_data[\"challenges\"][order_dict[challenge]][\"block_data\"][\"scaled_frontier\"])[0]\n",
    "    else:\n",
    "        difficulty = unscaled_difficulties[challenge_id_dict[challenge][0]]\n",
    "    with open(\"settings.json\", \"w+\") as file:\n",
    "            settings = '{' + f'\"block_id\": \"\",\"algorithm_id\": \"\",\"challenge_id\": \"\",\"player_id\": \"\",\"difficulty\": {difficulty}' + '}'\n",
    "            file.write(settings)\n",
    "\n",
    "    workers = []\n",
    "    testing = []\n",
    "    \n",
    "    #looping through all available algorithms for benchmarking in \n",
    "    for algorithm in challenge_id_dict[challenge][1:]:\n",
    "\n",
    "        #Downloading wasm blob\n",
    "        branch_name = f\"{challenge_id_dict[challenge][0]}/{algorithm_id_dict[algorithm]}\"\n",
    "        url = f\"https://raw.githubusercontent.com/tig-foundation/tig-monorepo/{branch_name}/tig-algorithms/wasm/{branch_name}.wasm\"\n",
    "        response = requests.get(url)\n",
    "        with open(f\"wasm/{algorithm_id_dict[algorithm]}.wasm\", \"wb\") as file:\n",
    "            file.write(response.content)\n",
    "\n",
    "        workers.append(multiprocessing.Process(target=run_test, args=(algorithm_id_dict[algorithm], )))\n",
    "        testing.append(algorithm_id_dict[algorithm])\n",
    "        if len(workers) == num_cores.result:\n",
    "            run_batch(workers)\n",
    "            print(f\"Testing batch of: {\", \".join(testing)} at difficulty: {difficulty}\")\n",
    "            workers = []\n",
    "            testing = []\n",
    "\n",
    "    if len(workers) != 0:\n",
    "        print(f\"Testing batch of: {\", \".join(testing)} at difficulty: {difficulty}\")\n",
    "        run_batch(workers)\n",
    "\n",
    "print(\"Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51099548-5a45-4e9e-9e84-90422d0a2de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filtering compiler messages and printing out results\n",
    "with open(\"test_results.txt\", \"r\") as file:\n",
    "    captured = file.readlines()\n",
    "\n",
    "test_data = [x for x in captured if \"solution\" in x]\n",
    "\n",
    "for challenge in challenge_id_dict.keys():\n",
    "\n",
    "    tested_algorithms = test_data[:(len(challenge_id_dict[challenge])-1)]\n",
    "    test_data = test_data[(len(challenge_id_dict[challenge])-1):]\n",
    "    \n",
    "    results_dict = {float(re.search(r\"(?:avg_time_per_solution: )(\\d*\\.?\\d+)(?:ms)\", test).groups()[0]):re.search(r\"(?:Algorithm: )(\\w+)(,)\", test).groups()[0] for test in tested_algorithms}\n",
    "    \n",
    "    try:\n",
    "        fastest_speed = min([x for x in results_dict.keys() if x != 0])\n",
    "    except ValueError:\n",
    "        print(f\"For {challenge_id_dict[challenge][0]}, no solutions were found using any algorithnms - try a lower difficulty or a longer testing timer\")\n",
    "    else:\n",
    "        best_algo = results_dict[fastest_speed]\n",
    "    \n",
    "        if difficulty_selector.result == \"scaled\":\n",
    "            difficulty = sorted(challenge_data[\"challenges\"][order_dict[challenge]][\"block_data\"][\"scaled_frontier\"])[0]\n",
    "        else:\n",
    "            difficulty = unscaled_difficulties[challenge_id_dict[challenge][0]]\n",
    "    \n",
    "        print(f\"Best algorithm for {challenge_id_dict[challenge][0]} is {best_algo}, with an avg_time_per_solution of {fastest_speed}ms, tested with difficulty: {difficulty}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
